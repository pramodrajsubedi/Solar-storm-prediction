# -*- coding: utf-8 -*-
"""solar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IyIf6bAwf1DRdz_9CvTmxrAUfKmMeN14

# New Section
"""

# General
import numpy as np
import pandas as pd
from pathlib import Path

# PyTorch
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# Scikit-learn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

# Plotting
import matplotlib.pyplot as plt

df = pd.read_csv( "/content/drive/MyDrive/Colab Notebooks/data/merged_symh_kpap.csv")
print(df.head())

df_2 = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/cleaned_merged_symh_kpap.csv")
print(df_2.head())
print(df_2.isnull().sum())

import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(df_2[['Total_IMF', 'BZ(GSM)', 'Velocity', 'Proton_density',
                'Proton_temperature', 'Flow_pressure', 'Ae_index']].isnull(),
            cbar=False, yticklabels=False)
plt.title("Missing Data Heatmap")
plt.show()


from sklearn.preprocessing import StandardScaler

selected_features = [
    'Total_IMF', 'BZ(GSM)', 'Velocity', 'Proton_density', 'Proton_temperature',
    'Flow_pressure', 'Ae_index',
    'Kp1', 'Kp2', 'Kp3', 'Kp4', 'Kp5', 'Kp6', 'Kp7', 'Kp8',
    'ap1', 'ap2', 'ap3', 'ap4', 'ap5', 'ap6', 'ap7', 'ap8',
    'Ap', 'SN', 'F10.7obs', 'F10.7adj'
]
features = df_2[selected_features]
target = df_2['SYM_H']

# Normalize
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X = scaler_X.fit_transform(features)
y = scaler_y.fit_transform(target.values.reshape(-1, 1))

# Convert to sequences for multi-step prediction
def create_sequences(X, y, window_size=24, horizon=3):
    X_seq, y_seq = [], []
    for i in range(len(X) - window_size - horizon):
        X_seq.append(X[i:i+window_size])
        y_seq.append(y[i+window_size:i+window_size+horizon].flatten())
    return np.array(X_seq), np.array(y_seq)

X_seq, y_seq = create_sequences(X, y, window_size=24, horizon=3)

X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class ASTGNN_LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, gnn_hidden, num_layers, horizon):
        super().__init__()
        self.gnn_layer = nn.Linear(input_size, gnn_hidden)
        self.lstm = nn.LSTM(input_size=gnn_hidden, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, horizon)

    def forward(self, x):
        x = self.gnn_layer(x)
        x, _ = self.lstm(x)
        x = self.fc(x[:, -1, :])
        return x

model = ASTGNN_LSTM(input_size=X_train.shape[2], hidden_size=128, gnn_hidden=64, num_layers=2, horizon=3).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()

train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)), batch_size=128, shuffle=True)

import torch

torch.cuda.empty_cache()

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

for epoch in range(95):
    model.train()
    total_loss = 0
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        optimizer.zero_grad()
        pred = model(batch_x)
        loss = loss_fn(pred, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Log the loss for each epoch
    writer.add_scalar('Loss/train', total_loss/len(train_loader), epoch)
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}")
writer.close()

model.eval()
with torch.no_grad():
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)
    y_pred = model(X_test_tensor).cpu().numpy()
    y_test_inv = scaler_y.inverse_transform(y_test_tensor.cpu().numpy())
    y_pred_inv = scaler_y.inverse_transform(y_pred)
    mse = mean_squared_error(y_test_inv, y_pred_inv)
    print("Test MSE:", mse)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Compute metrics
mse = mean_squared_error(y_test_inv, y_pred_inv)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test_inv, y_pred_inv)
r2 = r2_score(y_test_inv, y_pred_inv)

print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"RÂ² Score: {r2:.3f}")

# Visualization
plt.figure(figsize=(10, 5))
plt.plot(y_test_inv[:200, 0], label='True SYM-H')
plt.plot(y_pred_inv[:200, 0], label='Predicted SYM-H')
plt.legend()
plt.title("SYM-H Prediction vs Ground Truth")
plt.xlabel("Time Steps")
plt.ylabel("SYM-H (nT)")
plt.grid(True)
plt.show()

